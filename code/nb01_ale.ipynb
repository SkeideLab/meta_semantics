{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Notebook #01: ALE analyses\n",
    "\n",
    "This first notebook computes coordinate-based meta-analyses using the activation likelihood estimation (ALE) algorithm (Eickhoff et al., 2009; 2012; Turkeltaub et al., 2002). It takes as its inputs a table with descriptive information about all included experiments (which is stored in a Pandas DataFrame), and the individual peak coordinates from these experiments (read from individual CSV-files and stored as 2D NumPy arrays within the DataFrame). It then writes these coordinates to a Sleuth text file (for an example see http://www.brainmap.org/ale/foci2.txt). This text file is fed into the ALE algorithm as implemented in the GingerALE software (see http://www.brainmap.org/ale/). This is once for all experiments included in the meta-analysis and for seperate subgroups of experiments (e.g. based on specific categories of semantic tasks or the mean age of the children).\n",
    "\n",
    "We start by reading the table of experiments."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "# Read table of included experiments\n",
    "import pandas as pd\n",
    "exps = pd.read_csv('../data/literature_search/included.csv', na_filter=False,\n",
    "                   converters={'age_mean': float, 'age_min': float, 'age_max': float})"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Next, we take care of the fact that two experiments don't report the mean age of the children in the article. To nevertheless be able to inlcude these experiments (when assessing the influence of age), we use the midpoint of the age range instead of the mean. We also compute the median accross the mean sample ages of all experiments, which we will use later on to perform a median split analysis (older vs. younger)."
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "# Fill in mean age if missing (replace with the midpoint of min and max)\n",
    "import numpy as np\n",
    "exps['age_mean'] = [np.mean([age_min, age_max])\n",
    "                    if np.isnan(age_mean) else age_mean\n",
    "                    for age_mean, age_min, age_max in zip(exps['age_mean'], exps['age_min'], exps['age_max'])]\n",
    "\n",
    "# Compute median of mean ages (for median split)\n",
    "age_md = exps['age_mean'].median()"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We can then read the peak coordinates from the individual CSV files for all experiments. When necessary, we convert coordinates reported in Talairach space to (common) MNI space."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Read peak coordinates from CSV files\n",
    "exps['fname'] = '../data/foci/' + exps['experiment'] + '.csv'\n",
    "exps['foci'] = [np.genfromtxt(fname, delimiter=\",\", skip_header=1)\n",
    "                for fname in exps['fname']]\n",
    "\n",
    "# Make sure all foci are stored as 2D NumPy arrays\n",
    "exps['foci'] = [np.expand_dims(foci, axis=0)\n",
    "                if np.ndim(foci) != 2 else foci\n",
    "                for foci in exps['foci']]\n",
    "\n",
    "# Convert from Talairach to MNI space if necessary\n",
    "from nimare.transforms import tal2mni\n",
    "exps['foci_mni'] = [tal2mni(foci[:,0:3])\n",
    "                    if foci_space == 'TAL' else foci[:,0:3]\n",
    "                    for foci, foci_space in zip(exps['foci'], exps['foci_space'])]"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We then need to create the Sleuth text files on which can be read by GingerALE to perform the actual ALE meta-analyses. We therefore define a function which takes as its input the experiments DataFrame and a query for subsetting it (if we want to perform the analysis on a subset of all experiments). We can provide this information as a dictionary together with the desired file names for the text files."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "# Define function to write a subset of the experiments to a Sleuth text file\n",
    "def write_foci(fname, df, query):\n",
    "    from os import makedirs, path\n",
    "    makedirs(path.dirname(fname), exist_ok=True)\n",
    "    f = open(file = fname, mode = 'w')\n",
    "    f.write('// Reference=MNI\\n')\n",
    "    f.close()\n",
    "    f = open(file = fname, mode = 'a')\n",
    "    df_sub = df.query(query)\n",
    "    for experiment, n, foci_mni in zip(df_sub['experiment'], df_sub['n'], df_sub['foci_mni']):\n",
    "        f.write('// ' + experiment + '\\n// Subjects=' + str(n) + '\\n')\n",
    "        np.savetxt(f, foci_mni, fmt='%1.3f', delimiter='\\t')\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "# Define dictionary for which ALE analyses to run\n",
    "ales = dict({'../results/ale/all.txt': 'experiment == experiment',\n",
    "             '../results/ale/knowledge.txt': 'task_type == \"knowledge\"',\n",
    "             '../results/ale/nknowledge.txt': 'task_type != \"knowledge\"',\n",
    "             '../results/ale/lexical.txt': 'task_type == \"lexical\"',\n",
    "             '../results/ale/nlexical.txt': 'task_type != \"lexical\"',\n",
    "             '../results/ale/objects.txt': 'task_type == \"objects\"',\n",
    "             '../results/ale/nobjects.txt': 'task_type != \"objects\"',\n",
    "             '../results/ale/older.txt': 'age_mean > @age_md',\n",
    "             '../results/ale/younger.txt': 'age_mean <= @age_md'})\n",
    "\n",
    "# Use the function to write the Sleuth files\n",
    "for key, value in zip(ales.keys(), ales.values()):\n",
    "    write_foci(fname=key, df=exps, query=value)"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "We are now ready to perform the actual ALE analyses using GingerALE. We write a custom function which wraps the command line GingerALE tasks for performing the actual estimation (including cluster-wise FWE-correction) and for extracting the descriptive information about the significant clusters. We apply this function to all the Sleuth text files which we have created above."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Performing ALE for ../results/ale/all.txt with 10 permutations\n",
      "Performing ALE for ../results/ale/knowledge.txt with 10 permutations\n",
      "Performing ALE for ../results/ale/nknowledge.txt with 10 permutations\n",
      "Performing ALE for ../results/ale/lexical.txt with 10 permutations\n",
      "Performing ALE for ../results/ale/nlexical.txt with 10 permutations\n",
      "Performing ALE for ../results/ale/objects.txt with 10 permutations\n",
      "Performing ALE for ../results/ale/nobjects.txt with 10 permutations\n",
      "Performing ALE for ../results/ale/older.txt with 10 permutations\n",
      "Performing ALE for ../results/ale/younger.txt with 10 permutations\n"
     ]
    }
   ],
   "source": [
    "# Perform the ALE analyses\n",
    "import os\n",
    "os.environ['ALE'] = 'java -cp ../software/ale/GingerALE.jar'\n",
    "\n",
    "# Define function to run the ALE analysis\n",
    "def run_ale(fname, p_voxel, p_cluster, perm):\n",
    "    print('Performing ALE for \"' + fname + '\" with ' + str(perm) + ' permutations')\n",
    "    # Run ALE from the command line\n",
    "    cmd_ale = '${ALE} org.brainmap.meta.getALE2 ' + fname + ' -mask=MNI152_wb.nii -p=' + \\\n",
    "          str(p_voxel) + ' -perm=' + str(perm) + ' -clust=' + str(p_cluster) + ' -nonAdd'\n",
    "    os.system(cmd_ale)\n",
    "    # Retrieve cluster stats\n",
    "    prefix = os.path.splitext(fname)[0]\n",
    "    perm_str = str(perm // 1000) + 'k' if perm >= 1000 else str(perm)\n",
    "    cmd_cluster = '${ALE} org.brainmap.meta.getClustersStats ' + fname + ' ' + prefix + '_ALE.nii ' + \\\n",
    "    prefix + '_p001_C01_' + perm_str + '_clust.nii -mni -p=' + prefix + '_PVal.nii -z=' + prefix + '_Z.nii'\n",
    "    os.system(cmd_cluster)\n",
    "\n",
    "\n",
    "# Use the function to performe the ALEs\n",
    "for key in ales.keys():\n",
    "    run_ale(fname=key, p_voxel=0.001, p_cluster=0.01, perm=1000)\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Finally, let's look at some exemplary results by plotting the (masked) *Z* map from the main ALE analysis (including all semantic experiments) and by reading the corresponding cluster table."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "File not found: '../results/ale/all_Z.nii'",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mValueError\u001B[0m                                Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-6-17947037b21a>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      1\u001B[0m \u001B[0;31m# Glass brain example (we first need to mask the Z map with the cluster map)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      2\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mnilearn\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mimage\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mplotting\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 3\u001B[0;31m \u001B[0mimg_z\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_img\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'../results/ale/all_Z.nii'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      4\u001B[0m \u001B[0mimg_mask\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mload_img\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'../results/ale/all_p001_C01_1k_clust.nii'\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      5\u001B[0m \u001B[0mimg_mask\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0mimage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mmath_img\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m'np.where(img > 0, 1, 0)'\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mimg\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mimg_mask\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/mask_children/lib/python3.7/site-packages/nilearn/image/image.py\u001B[0m in \u001B[0;36mload_img\u001B[0;34m(img, wildcards, dtype)\u001B[0m\n\u001B[1;32m   1098\u001B[0m         \u001B[0mnilearn\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mimage\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mget_data\u001B[0m \u001B[0mreturns\u001B[0m \u001B[0mits\u001B[0m \u001B[0mdata\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1099\u001B[0m     \"\"\"\n\u001B[0;32m-> 1100\u001B[0;31m     \u001B[0;32mreturn\u001B[0m \u001B[0mcheck_niimg\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mimg\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mwildcards\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mwildcards\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdtype\u001B[0m\u001B[0;34m=\u001B[0m\u001B[0mdtype\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m   1101\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m   1102\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/opt/anaconda3/envs/mask_children/lib/python3.7/site-packages/nilearn/_utils/niimg_conversions.py\u001B[0m in \u001B[0;36mcheck_niimg\u001B[0;34m(niimg, ensure_ndim, atleast_4d, dtype, return_iterator, wildcards)\u001B[0m\n\u001B[1;32m    249\u001B[0m                 \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mmessage\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    250\u001B[0m             \u001B[0;32melse\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m--> 251\u001B[0;31m                 \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"File not found: '%s'\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mniimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m    252\u001B[0m         \u001B[0;32melif\u001B[0m \u001B[0;32mnot\u001B[0m \u001B[0mos\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpath\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mexists\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0mniimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m:\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m    253\u001B[0m             \u001B[0;32mraise\u001B[0m \u001B[0mValueError\u001B[0m\u001B[0;34m(\u001B[0m\u001B[0;34m\"File not found: '%s'\"\u001B[0m \u001B[0;34m%\u001B[0m \u001B[0mniimg\u001B[0m\u001B[0;34m)\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mValueError\u001B[0m: File not found: '../results/ale/all_Z.nii'"
     ]
    }
   ],
   "source": [
    "# Glass brain example (we first need to mask the Z map with the cluster map)\n",
    "from nilearn import image, plotting\n",
    "img_z = image.load_img('../results/ale/all_Z.nii')\n",
    "img_mask = image.load_img('../results/ale/all_p001_C01_1k_clust.nii')\n",
    "img_mask = image.math_img('np.where(img > 0, 1, 0)', img=img_mask)\n",
    "img_masked = image.math_img('img1*img2', img1=img_z, img2=img_mask)\n",
    "p = plotting.plot_glass_brain(img_masked, display_mode='lyrz', vmin=0, vmax=8, colorbar=False)\n",
    "\n",
    "# Cluster table example\n",
    "pd.read_csv('../results/ale/all_clust.xls')\n"
   ],
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%%\n"
    }
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "name": "pycharm-2e5bb5f0",
   "language": "python",
   "display_name": "PyCharm (mask_children)"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}