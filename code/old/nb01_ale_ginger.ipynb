{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Notebook #01: ALE analyses\n",
    "\n",
    "This first notebook computes coordinate-based meta-analyses using the activation likelihood estimation (ALE) algorithm (Eickhoff et al., 2009; 2012; Turkeltaub et al., 2002). It takes as its inputs a table with descriptive information about all included experiments (which is stored in a Pandas DataFrame), and the individual peak coordinates from these experiments (read from individual CSV-files and stored as 2D NumPy arrays within the DataFrame). It then writes these coordinates to a Sleuth text file (for an example see http://www.brainmap.org/ale/foci2.txt). This text file is fed into the ALE algorithm as implemented in the GingerALE software (see http://www.brainmap.org/ale/). This is once for all experiments included in the meta-analysis and for seperate subgroups of experiments (e.g. based on specific categories of semantic tasks or the mean age of the children).\n",
    "\n",
    "We start by reading the table of experiments."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-28T17:15:34.070252Z",
     "iopub.status.busy": "2020-12-28T17:15:34.069178Z",
     "iopub.status.idle": "2020-12-28T17:15:34.609247Z",
     "shell.execute_reply": "2020-12-28T17:15:34.609804Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read table of included experiments\n",
    "import pandas as pd\n",
    "exps = pd.read_csv('../data/literature_search/included.csv', na_filter=False,\n",
    "                   converters={'age_mean': float, 'age_min': float, 'age_max': float})"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false,
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Next, we take care of the fact that two experiments don't report the mean age of the children in the article. To nevertheless be able to inlcude these experiments (when assessing the influence of age), we use the midpoint of the age range instead of the mean. We also compute the median accross the mean sample ages of all experiments, which we will use later on to perform a median split analysis (older vs. younger)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-28T17:15:34.616090Z",
     "iopub.status.busy": "2020-12-28T17:15:34.615408Z",
     "iopub.status.idle": "2020-12-28T17:15:34.617488Z",
     "shell.execute_reply": "2020-12-28T17:15:34.617987Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Fill in mean age if missing (replace with the midpoint of min and max)\n",
    "import numpy as np\n",
    "exps['age_mean'] = [np.mean([age_min, age_max])\n",
    "                    if np.isnan(age_mean) else age_mean\n",
    "                    for age_mean, age_min, age_max\n",
    "                    in zip(exps['age_mean'], exps['age_min'], exps['age_max'])]\n",
    "\n",
    "# Compute median of mean ages (for median split)\n",
    "age_md = exps['age_mean'].median()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We can then read the peak coordinates from the individual CSV files for all experiments. When necessary, we convert coordinates reported in Talairach space to (common) MNI space."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-28T17:15:34.623292Z",
     "iopub.status.busy": "2020-12-28T17:15:34.622805Z",
     "iopub.status.idle": "2020-12-28T17:15:36.856998Z",
     "shell.execute_reply": "2020-12-28T17:15:36.857675Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Read peak coordinates from CSV files\n",
    "exps['fname'] = '../data/foci/' + exps['experiment'] + '.csv'\n",
    "exps['foci'] = [np.genfromtxt(fname, delimiter=\",\", skip_header=1)\n",
    "                for fname in exps['fname']]\n",
    "\n",
    "# Make sure all foci are stored as 2D NumPy arrays\n",
    "exps['foci'] = [np.expand_dims(foci, axis=0)\n",
    "                if np.ndim(foci) != 2 else foci\n",
    "                for foci in exps['foci']]\n",
    "\n",
    "# Convert from Talairach to MNI space if necessary\n",
    "from nimare.transforms import tal2mni\n",
    "exps['foci_mni'] = [tal2mni(foci[:,0:3])\n",
    "                    if foci_space == 'TAL' else foci[:,0:3]\n",
    "                    for foci, foci_space in zip(exps['foci'], exps['foci_space'])]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We then need to create the Sleuth text files on which can be read by GingerALE to perform the actual ALE meta-analyses. We therefore define a function which takes as its input the experiments DataFrame and a query for subsetting it (if we want to perform the analysis on a subset of all experiments). We can provide this information as a dictionary together with the desired file names for the text files."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-28T17:15:36.865161Z",
     "iopub.status.busy": "2020-12-28T17:15:36.864474Z",
     "iopub.status.idle": "2020-12-28T17:15:36.916874Z",
     "shell.execute_reply": "2020-12-28T17:15:36.917592Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define function to write a subset of the experiments to a Sleuth text file\n",
    "def write_foci(fname, df, query):\n",
    "    from os import makedirs, path\n",
    "    makedirs(path.dirname(fname), exist_ok=True)\n",
    "    f = open(file = fname, mode = 'w')\n",
    "    f.write('// Reference=MNI\\n')\n",
    "    f.close()\n",
    "    f = open(file = fname, mode = 'a')\n",
    "    df_sub = df.query(query)\n",
    "    for experiment, n, foci_mni in zip(df_sub['experiment'], df_sub['n'], df_sub['foci_mni']):\n",
    "        f.write('// ' + experiment + '\\n// Subjects=' + str(n) + '\\n')\n",
    "        np.savetxt(f, foci_mni, fmt='%1.3f', delimiter='\\t')\n",
    "        f.write('\\n')\n",
    "    f.close()\n",
    "\n",
    "\n",
    "# Define dictionary for which ALE analyses to run\n",
    "ales = dict({'../results/ale/all.txt': 'experiment == experiment',\n",
    "             '../results/ale/knowledge.txt': 'task_type == \"knowledge\"',\n",
    "             '../results/ale/nknowledge.txt': 'task_type != \"knowledge\"',\n",
    "             '../results/ale/lexical.txt': 'task_type == \"lexical\"',\n",
    "             '../results/ale/nlexical.txt': 'task_type != \"lexical\"',\n",
    "             '../results/ale/objects.txt': 'task_type == \"objects\"',\n",
    "             '../results/ale/nobjects.txt': 'task_type != \"objects\"',\n",
    "             '../results/ale/older.txt': 'age_mean > @age_md',\n",
    "             '../results/ale/younger.txt': 'age_mean <= @age_md'})\n",
    "\n",
    "# Use the function to write the Sleuth files\n",
    "for key, value in zip(ales.keys(), ales.values()):\n",
    "    write_foci(fname=key, df=exps, query=value)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "We are now ready to perform the actual ALE analyses using GingerALE. We write a custom function which wraps the command line GingerALE tasks for performing the actual estimation (including cluster-wise FWE-correction) and for extracting the descriptive information about the significant clusters. We apply this function to all the Sleuth text files which we have created above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-28T17:15:36.927802Z",
     "iopub.status.busy": "2020-12-28T17:15:36.926944Z",
     "iopub.status.idle": "2020-12-28T17:17:19.639727Z",
     "shell.execute_reply": "2020-12-28T17:17:19.640598Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Define function to run a single ALE analysis\n",
    "def run_ale(fname, gingerale, p_voxel, p_cluster, perm):\n",
    "    print('Performing ALE for \"' + fname + '\" with ' + str(perm) + ' permutations\\n')\n",
    "    # Run ALE from the command line\n",
    "    from subprocess import Popen\n",
    "    cmd_ale = 'java -cp ' + gingerale + ' org.brainmap.meta.getALE2 ' + fname + \\\n",
    "              ' -mask=MNI152_wb.nii -p=' + str(p_voxel) + ' -perm=' + str(perm) + \\\n",
    "              ' -clust=' + str(p_cluster) + ' -nonAdd'\n",
    "    Popen(cmd_ale, shell=True).wait()\n",
    "    # Retrieve cluster stats\n",
    "    from os.path import splitext\n",
    "    prefix =  splitext(fname)[0]\n",
    "    perm_str = str(perm // 1000) + 'k' if perm >= 1000 else str(perm)\n",
    "    cmd_cluster = 'java -cp ' + gingerale + ' org.brainmap.meta.getClustersStats ' + fname + \\\n",
    "                  ' ' + prefix + '_ALE.nii ' + prefix + '_p001_C01_' + perm_str + \\\n",
    "                  '_clust.nii -mni -p=' + prefix + '_PVal.nii -z=' + prefix + '_Z.nii'\n",
    "    Popen(cmd_cluster, shell=True).wait()\n",
    "\n",
    "\n",
    "# Set up multiprocessing\n",
    "def run_ales_parallel(fnames, gingerale, p_voxel, p_cluster, perm):\n",
    "    from multiprocessing import Pool, cpu_count\n",
    "    from functools import partial\n",
    "    processes = min(len(fnames), cpu_count())\n",
    "    print('Starting ALE analyses in parellel using ' + str(processes) + ' processes:')\n",
    "    pool = Pool(processes=processes)\n",
    "    func = partial(run_ale, gingerale=gingerale, p_voxel=p_voxel, p_cluster=p_cluster, perm=perm)\n",
    "    pool.map(func, fnames)\n",
    "    pool.close()\n",
    "    pool.join()\n",
    "\n",
    "\n",
    "# Run the ALEs\n",
    "if __name__ == '__main__':\n",
    "    run_ales_parallel(fnames=ales.keys(), gingerale='../software/ale/GingerALE.jar',\n",
    "                      p_voxel=0.001, p_cluster=0.01, perm=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": false
   },
   "source": [
    "Finally, let's look at some exemplary results by plotting the (masked) ALE map from the main analysis (including all semantic experiments) and by reading the corresponding cluster table."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2020-12-28T17:17:19.650611Z",
     "iopub.status.busy": "2020-12-28T17:17:19.649815Z",
     "iopub.status.idle": "2020-12-28T17:17:21.775140Z",
     "shell.execute_reply": "2020-12-28T17:17:21.775714Z"
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Glass brain example\n",
    "from glob import glob\n",
    "from nilearn import image, plotting\n",
    "img = image.load_img(glob('../results/ale/all_p*_ALE.nii'))\n",
    "p = plotting.plot_glass_brain(stat_map_img=img, display_mode='lyrz',\n",
    "                              vmin=0, vmax=0.05, colorbar=True)\n",
    "\n",
    "# Cluster table example\n",
    "clusts = pd.read_csv('../results/ale/all_clust.xls', delimiter='\\t')\n",
    "clusts.style.format({'ALE': '{:.3f}', 'P': '{:.3f}', 'Z': '{:.2f}'})\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "PyCharm (mask_children)",
   "language": "python",
   "name": "pycharm-2e5bb5f0"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}